{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Bf5HG3LgJpO",
    "outputId": "3ac651c8-d096-4e8a-e614-2c417ac06de3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9166404529726329\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Not Hate Speech       0.38      0.50      0.43       406\n",
      "    Hate Speech       0.97      0.94      0.96      5952\n",
      "\n",
      "       accuracy                           0.92      6358\n",
      "      macro avg       0.67      0.72      0.69      6358\n",
      "   weighted avg       0.93      0.92      0.92      6358\n",
      "\n",
      "\n",
      "Tweet: i hate this so much this is terrible\n",
      "Prediction: Hate Speech\n",
      "\n",
      "Tweet: wow this is amazing great job\n",
      "Prediction: Hate Speech\n",
      "\n",
      "Tweet: you are so dumb i cant believe this\n",
      "Prediction: Not Hate Speech\n",
      "\n",
      "Tweet: wishing everyone a happy and peaceful day\n",
      "Prediction: Hate Speech\n",
      "\n",
      "Tweet: this is absolutely disgusting and unacceptable\n",
      "Prediction: Hate Speech\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"HateSpeechDetection\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "df = spark.read.csv(\"twitter_practical(4A).csv\", header=True, inferSchema=True)\n",
    "df = df.withColumnRenamed(\"tweet\", \"text\").withColumnRenamed(\"label\", \"category\")\n",
    "df = df.dropna(subset=[\"text\", \"category\"])\n",
    "\n",
    "# Convert labels (0: Hate → 1, 1/2: Not Hate → 0)\n",
    "label_udf = udf(lambda x: 1 if x == 0 else 0, IntegerType())\n",
    "df = df.withColumn(\"label\", label_udf(col(\"category\")))\n",
    "\n",
    "# Clean text\n",
    "def clean_text(df):\n",
    "    df = df.withColumn(\"text\", lower(col(\"text\")))\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), r\"http\\S+\", \"\"))\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), r\"@\\w+\", \"\"))\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), r\"#\\w+\", \"\"))\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), r\"[^a-zA-Z\\s]\", \"\"))\n",
    "    return df\n",
    "\n",
    "df = clean_text(df)\n",
    "\n",
    "# Build pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=5000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])\n",
    "\n",
    "# Split\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "model = pipeline.fit(train_data)\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Classification report\n",
    "y_true = predictions.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "y_pred = predictions.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Not Hate Speech\", \"Hate Speech\"]))\n",
    "\n",
    "# -------------------------------\n",
    "# Test custom tweets\n",
    "# -------------------------------\n",
    "sample_tweets = [\n",
    "    (\"I hate this so much! This is terrible!\",),\n",
    "    (\"Wow, this is amazing! Great job!\",),\n",
    "    (\"You are so dumb, I can't believe this!\",),\n",
    "    (\"Wishing everyone a happy and peaceful day!\",),\n",
    "    (\"This is absolutely disgusting and unacceptable!\",)\n",
    "]\n",
    "\n",
    "sample_df = spark.createDataFrame(sample_tweets, [\"text\"])\n",
    "sample_df = clean_text(sample_df)  # Clean same as training data\n",
    "\n",
    "# Predict\n",
    "results = model.transform(sample_df).select(\"text\", \"prediction\")\n",
    "\n",
    "# Show predictions\n",
    "for row in results.collect():\n",
    "    tweet = row[\"text\"]\n",
    "    prediction = \"Hate Speech\" if row[\"prediction\"] == 1.0 else \"Not Hate Speech\"\n",
    "    print(f\"\\nTweet: {tweet}\\nPrediction: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0XHlB4MFtuu",
    "outputId": "c0e36a98-ef6d-42e1-8a59-efdd687b5ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution Time: 16.62 seconds\n",
      "+-----------------------------------+----------+---------+\n",
      "|tweet                              |prediction|sentiment|\n",
      "+-----------------------------------+----------+---------+\n",
      "|i hate this product its the worst  |1.0       |Negative |\n",
      "|this is amazing i love it          |0.0       |Positive |\n",
      "|not bad but could be better        |0.0       |Positive |\n",
      "|absolutely terrible experience     |1.0       |Negative |\n",
      "|had a fantastic time using this app|0.0       |Positive |\n",
      "+-----------------------------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, when, rand\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"FixedSentimentAnalysis\").getOrCreate()\n",
    "start_time = time.time()\n",
    "\n",
    "# Load dataset\n",
    "df = spark.read.csv(\"twitter.csv\", header=True, inferSchema=True).select(\"tweet\", \"label\")\n",
    "\n",
    "# Clean tweets\n",
    "df_clean = df.withColumn(\"tweet\", lower(col(\"tweet\")))\n",
    "df_clean = df_clean.withColumn(\"tweet\", regexp_replace(\"tweet\", r\"http\\S+|www\\S+\", \"\"))\n",
    "df_clean = df_clean.withColumn(\"tweet\", regexp_replace(\"tweet\", r\"@\\w+\", \"\"))\n",
    "df_clean = df_clean.withColumn(\"tweet\", regexp_replace(\"tweet\", r\"#\", \"\"))\n",
    "df_clean = df_clean.withColumn(\"tweet\", regexp_replace(\"tweet\", r\"[^\\w\\s]\", \"\"))\n",
    "df_clean = df_clean.withColumn(\"tweet\", regexp_replace(\"tweet\", r\"\\d+\", \"\"))\n",
    "\n",
    "# Handle class imbalance\n",
    "positive = df_clean.filter(col(\"label\") == 1)\n",
    "negative = df_clean.filter(col(\"label\") == 0)\n",
    "neg_sample = negative.sample(False, positive.count() / negative.count(), seed=42)\n",
    "df_train = neg_sample.union(positive).orderBy(rand())\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    (\"I hate this product! It's the worst!\",),\n",
    "    (\"This is amazing! I love it.\",),\n",
    "    (\"Not bad, but could be better.\",),\n",
    "    (\"Absolutely terrible experience.\",),\n",
    "    (\"Had a fantastic time using this app.\",),\n",
    "]\n",
    "df_test = spark.createDataFrame(test_sentences, [\"tweet\"])\n",
    "\n",
    "# Clean test tweets\n",
    "for pattern in [r\"http\\S+|www\\S+\", r\"@\\w+\", r\"#\", r\"[^\\w\\s]\", r\"\\d+\"]:\n",
    "    df_test = df_test.withColumn(\"tweet\", regexp_replace(\"tweet\", pattern, \"\"))\n",
    "df_test = df_test.withColumn(\"tweet\", lower(col(\"tweet\")))\n",
    "\n",
    "# ML Pipeline\n",
    "tokenizer = RegexTokenizer(inputCol=\"tweet\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\", vocabSize=10000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")  # <-- use original label\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, vectorizer, idf, lr])\n",
    "\n",
    "# Train model\n",
    "model = pipeline.fit(df_train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(df_test)\n",
    "\n",
    "# Map prediction\n",
    "predictions = predictions.withColumn(\n",
    "    \"sentiment\",\n",
    "    when(col(\"prediction\") == 1.0, \"Negative\").otherwise(\"Positive\")\n",
    ")\n",
    "\n",
    "print(f\"\\nExecution Time: {time.time() - start_time:.2f} seconds\")\n",
    "predictions.select(\"tweet\", \"prediction\", \"sentiment\").show(truncate=False)\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vP7wDaKsQAEg",
    "outputId": "022d7b7a-9c4b-48b5-ebf1-f41376cdfc9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('anjali', 2), ('anushka', 2), ('poonam', 2), ('tejaswini', 1), ('harshal', 1), ('supriya', 1), ('ankush', 1)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext(\"local\", \"WordCount\")\n",
    "\n",
    "# Load text file into RDD\n",
    "text_rdd = sc.textFile(\"input.txt\")\n",
    "\n",
    "# Split the text into words, and map them to (word, 1)\n",
    "words_rdd = text_rdd.flatMap(lambda line: line.split(\" \")) \\\n",
    "                    .map(lambda word: (word, 1))\n",
    "\n",
    "# Reduce by key (word) to count occurrences\n",
    "word_count_rdd = words_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Collect and print the results\n",
    "print(word_count_rdd.collect())\n",
    "\n",
    "# Stop SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fCJJcgFS2vH",
    "outputId": "e68bf671-1c2d-4077-f045-9f88d3a931a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) -> 170\n",
      "(0, 1) -> 188\n",
      "(1, 0) -> 124\n",
      "(1, 1) -> 138\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MatrixMultiplication\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Example matrices\n",
    "matrix_A = [\n",
    "    (0, 0, 4), (0, 1, 6), (0, 2, 8),\n",
    "    (1, 0, 5), (1, 1, 5), (1, 2, 4)\n",
    "]\n",
    "\n",
    "matrix_B = [\n",
    "    (0, 0, 7), (0, 1, 8),\n",
    "    (1, 0, 9), (1, 1, 10),\n",
    "    (2, 0, 11), (2, 1, 12)\n",
    "]\n",
    "\n",
    "# Convert matrices into RDDs\n",
    "rdd_A = sc.parallelize(matrix_A)  # (row, col, value)\n",
    "rdd_B = sc.parallelize(matrix_B)  # (row, col, value)\n",
    "\n",
    "# Map phase: Convert matrix entries into (key, value) pairs\n",
    "mapped_A = rdd_A.map(lambda x: (x[1], (x[0], x[2])))  # Keyed by column of A\n",
    "mapped_B = rdd_B.map(lambda x: (x[0], (x[1], x[2])))  # Keyed by row of B\n",
    "\n",
    "# Join on common key (column index of A and row index of B)\n",
    "joined = mapped_A.join(mapped_B)\n",
    "# Result: (shared_index, ((row_A, val_A), (col_B, val_B)))\n",
    "\n",
    "# Compute partial products\n",
    "partial_products = joined.map(\n",
    "    lambda x: ((x[1][0][0], x[1][1][0]), x[1][0][1] * x[1][1][1])\n",
    ")\n",
    "\n",
    "# Reduce phase: Sum partial products for each (row, col) position\n",
    "result = partial_products.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Collect and print results\n",
    "output = result.collect()\n",
    "for ((row, col), value) in sorted(output):\n",
    "    print(f\"({row}, {col}) -> {value}\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5JxYLVaS_NL",
    "outputId": "330baa16-6cfa-42f3-d4ae-d6fd0ecfb544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: ./weather_data/2021/99495199999.csv\n",
      "Downloaded: ./weather_data/2021/72429793812.csv\n",
      "Downloaded: ./weather_data/2022/99495199999.csv\n",
      "Downloaded: ./weather_data/2022/72429793812.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2021/99495199999.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2021/72429793812.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2022/99495199999.csv\n",
      "Cleaned data saved to: ./cleaned_weather_data/2022/72429793812.csv\n",
      "Year: 2021, Station: 99495199999, Row count: 0\n",
      "Year: 2021, Station: 72429793812, Row count: 365\n",
      "Year: 2022, Station: 99495199999, Row count: 0\n",
      "Year: 2022, Station: 72429793812, Row count: 364\n",
      "+-----+------------------+------+----+------------------+\n",
      "|MONTH|              Mean|Median|Mode|Standard Deviation|\n",
      "+-----+------------------+------+----+------------------+\n",
      "|    1|  33.9516129032258|  34.8|33.1| 4.899583041289802|\n",
      "|    2| 29.95357142857143|  26.8|21.5| 9.450592070139862|\n",
      "|    3|  47.8258064516129|  47.7|48.2| 7.747987598593389|\n",
      "|    4| 52.87666666666666|  53.1|53.1| 8.798341667152622|\n",
      "|    5|60.858064516129026|  58.5|70.9| 8.565931519437857|\n",
      "|    6| 72.87666666666668|  72.9|63.5| 4.985128458437801|\n",
      "|    7| 74.77419354838709|  75.2|73.9|  3.45571678449257|\n",
      "|    8| 75.52258064516128|  75.6|75.5| 3.920349446789945|\n",
      "|    9| 68.51333333333335|  69.7|74.9| 5.616400450732527|\n",
      "|   10| 61.18387096774194|  61.4|60.0| 7.731541319995368|\n",
      "|   11| 41.54666666666666|  39.1|37.7| 6.540838991493651|\n",
      "|   12| 43.85161290322581|  46.1|32.7| 9.586896298863428|\n",
      "+-----+------------------+------+----+------------------+\n",
      "\n",
      "+----------+-------------------+\n",
      "|      DATE|         Wind Chill|\n",
      "+----------+-------------------+\n",
      "|2022-12-24|-11.578195940683953|\n",
      "|2022-12-25| 0.5826037095438092|\n",
      "|2022-01-07|  6.093440406554773|\n",
      "|2022-01-29|  7.676316538955902|\n",
      "|2022-12-26|  9.987699442545647|\n",
      "|2022-01-26| 10.541917590830776|\n",
      "|2022-01-22|  12.31044410937974|\n",
      "|2022-02-05| 12.337344983780074|\n",
      "|2022-01-21| 12.363295778348393|\n",
      "|2022-01-06|  13.45395547721534|\n",
      "+----------+-------------------+\n",
      "\n",
      "Number of days with extreme weather conditions in Florida: 0\n",
      "+-----+------------------+\n",
      "|MONTH|Max Predicted Temp|\n",
      "+-----+------------------+\n",
      "|   11| 65.22373881977461|\n",
      "|   12| 53.08328332169012|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, month, dayofyear, max as spark_max, when, expr\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define the base URLs for the data\n",
    "base_url_1 = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/{}/99495199999.csv\"\n",
    "base_url_2 = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/{}/72429793812.csv\"\n",
    "\n",
    "# Define the range of years\n",
    "years = range(2021, 2023)\n",
    "\n",
    "# Base directory to save the downloaded files\n",
    "base_output_dir = \"./weather_data/\"\n",
    "\n",
    "# Download data for each year and station\n",
    "for year in years:\n",
    "    year_dir = os.path.join(base_output_dir, str(year))\n",
    "    os.makedirs(year_dir, exist_ok=True)\n",
    "\n",
    "    for base_url, station_id in [(base_url_1, \"99495199999\"), (base_url_2, \"72429793812\")]:\n",
    "        url = base_url.format(year)\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            file_path = os.path.join(year_dir, f\"{station_id}.csv\")\n",
    "            with open(file_path, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Downloaded: {file_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "\n",
    "# Clean the data by filtering out invalid values\n",
    "base_input_dir = \"./weather_data/\"\n",
    "base_output_dir = \"./cleaned_weather_data/\"\n",
    "\n",
    "invalid_values = {\n",
    "    \"MXSPD\": 999.9,\n",
    "    \"MAX\": 9999.9,\n",
    "    \"MIN\": 9999.9,\n",
    "}\n",
    "\n",
    "for year in range(2021, 2023):\n",
    "    year_dir = os.path.join(base_input_dir, str(year))\n",
    "\n",
    "    if os.path.exists(year_dir):\n",
    "        for station_id in [\"99495199999\", \"72429793812\"]:\n",
    "            file_path = os.path.join(year_dir, f\"{station_id}.csv\")\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path)\n",
    "                for column, invalid_value in invalid_values.items():\n",
    "                    df = df[df[column] != invalid_value]\n",
    "\n",
    "                output_year_dir = os.path.join(base_output_dir, str(year))\n",
    "                os.makedirs(output_year_dir, exist_ok=True)\n",
    "                cleaned_file_path = os.path.join(output_year_dir, f\"{station_id}.csv\")\n",
    "                df.to_csv(cleaned_file_path, index=False)\n",
    "                print(f\"Cleaned data saved to: {cleaned_file_path}\")\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Year directory not found: {year_dir}\")\n",
    "\n",
    "# Spark session setup\n",
    "spark = SparkSession.builder.appName(\"Weather Analysis\").getOrCreate()\n",
    "\n",
    "# Question 2: Load the CSV files and display the count of each dataset.\n",
    "for year in range(2021, 2023):\n",
    "    for station_id in [\"99495199999\", \"72429793812\"]:\n",
    "        file_path = os.path.join(base_output_dir, str(year), f\"{station_id}.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "            print(f\"Year: {year}, Station: {station_id}, Row count: {df.count()}\")\n",
    "\n",
    "# Question 6: Count missing values for 'GUST' in 2024\n",
    "base_path = \"./cleaned_weather_data/2024/\"\n",
    "station_codes = ['99495199999', '72429793812']\n",
    "results = []\n",
    "\n",
    "for station_code in station_codes:\n",
    "    file_path = os.path.join(base_path, f\"{station_code}.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        total_count = df.count()\n",
    "        missing_count = df.filter(df.GUST == 999.9).count()\n",
    "\n",
    "        missing_percentage = (missing_count / total_count) * 100 if total_count > 0 else 0.0\n",
    "        results.append((station_code, missing_percentage))\n",
    "\n",
    "for station_code, missing_percentage in results:\n",
    "    print(f\"Station Code: {station_code}, Missing GUST Percentage in 2024: {missing_percentage:.2f}%\")\n",
    "\n",
    "# Question 7: Mean, median, mode, and standard deviation for Cincinnati temperature in 2020\n",
    "df = spark.read.csv(\"./cleaned_weather_data/2021/72429793812.csv\", header=True, inferSchema=True)\n",
    "df_cincinnati = df.withColumn(\"MONTH\", month(col(\"DATE\")))\n",
    "result = df_cincinnati.groupBy(\"MONTH\").agg(\n",
    "    expr(\"mean(TEMP)\").alias(\"Mean\"),\n",
    "    expr(\"percentile_approx(TEMP, 0.5)\").alias(\"Median\"),\n",
    "    expr(\"mode(TEMP)\").alias(\"Mode\"),\n",
    "    expr(\"stddev(TEMP)\").alias(\"Standard Deviation\")\n",
    ")\n",
    "\n",
    "result.orderBy(\"MONTH\").show()\n",
    "\n",
    "# Question 8: Top 10 lowest Wind Chill days in Cincinnati 2017\n",
    "df = spark.read.csv(\"./cleaned_weather_data/2022/72429793812.csv\", header=True, inferSchema=True)\n",
    "df_cincinnati = df.filter((col(\"TEMP\") < 50) & (col(\"WDSP\") > 3))\n",
    "\n",
    "df_cincinnati = df_cincinnati.withColumn(\n",
    "    \"Wind Chill\",\n",
    "    35.74 + (0.6215 * col(\"TEMP\")) - (35.75 * (col(\"WDSP\") ** 0.16)) + (0.4275 * col(\"TEMP\") * (col(\"WDSP\") ** 0.16))\n",
    ")\n",
    "\n",
    "df_cincinnati = df_cincinnati.withColumn(\"DATE\", expr(\"date_format(DATE, 'yyyy-MM-dd')\"))\n",
    "result = df_cincinnati.select(\"DATE\", \"Wind Chill\").orderBy(\"Wind Chill\").limit(10)\n",
    "result.show()\n",
    "\n",
    "# Question 9: Count extreme weather days for Florida\n",
    "base_directory = './cleaned_weather_data/'\n",
    "file_paths = []\n",
    "for year in range(2015, 2025):\n",
    "    file_path = os.path.join(base_directory, str(year), '99495199999.csv')\n",
    "    if os.path.exists(file_path):\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "df = spark.read.csv(file_paths, header=True, inferSchema=True)\n",
    "extreme_weather_count = df.filter(col(\"FRSHTT\") != 0).count()\n",
    "print(f\"Number of days with extreme weather conditions in Florida: {extreme_weather_count}\")\n",
    "\n",
    "# Question 10: Predict max Temperature for Cincinnati in Nov and Dec 2024\n",
    "base_directory = './cleaned_weather_data'\n",
    "file_paths = []\n",
    "\n",
    "for year in [2022, 2023]:\n",
    "    file_path = os.path.join(base_directory, str(year), '72429793812.csv')\n",
    "    if os.path.exists(file_path):\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "historical_data = spark.read.csv(file_paths, header=True, inferSchema=True)\n",
    "historical_df = historical_data.filter(\n",
    "    (col(\"STATION\") == \"72429793812\") & (month(\"DATE\").isin([11, 12]))\n",
    ")\n",
    "\n",
    "training_data = historical_df.withColumn(\"DAY_OF_YEAR\", dayofyear(\"DATE\"))\n",
    "assembler = VectorAssembler(inputCols=[\"DAY_OF_YEAR\"], outputCol=\"features\")\n",
    "train_data = assembler.transform(training_data).select(\"features\", col(\"MAX\").alias(\"label\"))\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "predictions_df = spark.createDataFrame([(day,) for day in range(305, 366)], [\"DAY_OF_YEAR\"])\n",
    "predictions = assembler.transform(predictions_df)\n",
    "predicted_temps = lr_model.transform(predictions)\n",
    "\n",
    "max_predictions = predicted_temps.withColumn(\n",
    "    \"MONTH\", when(col(\"DAY_OF_YEAR\") < 335, 11).otherwise(12)\n",
    ").groupBy(\"MONTH\").agg(spark_max(\"prediction\").alias(\"Max Predicted Temp\"))\n",
    "\n",
    "max_predictions.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "fXi6fbyeU3zw",
    "outputId": "6323ef43-0ed7-4b5a-a512-ca1a7451ccfd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-4ad77abb-d22e-474f-9bb6-6878e66d5efc\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-4ad77abb-d22e-474f-9bb6-6878e66d5efc\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle (1).json\n"
     ]
    }
   ],
   "source": [
    "# Upload kaggle.json from local\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "\n",
    "# Move it to correct location\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "FpPZW7yBUj0f",
    "outputId": "f5ec05f4-f0d2-4d9a-86d9-84a70113ba50"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6f44ba7f8c49>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-6f44ba7f8c49>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Set up Kaggle API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKaggleApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Download dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\u001b[0m in \u001b[0;36mauthenticate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         raise IOError('Could not find {}. Make sure it\\'s located in'\n\u001b[0m\u001b[1;32m    434\u001b[0m                       \u001b[0;34m' {}. Or use the environment method. See setup'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                       \u001b[0;34m' instructions at'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "def parse_line(line):\n",
    "    \"\"\"Parses each line of input data into (movie_id, rating).\"\"\"\n",
    "    if line.startswith(\"userId,movieId,rating,timestamp\"):\n",
    "        return None\n",
    "    parts = line.split(\",\")\n",
    "    return (int(parts[1]), float(parts[2]))\n",
    "\n",
    "def main():\n",
    "    # Set up Kaggle API\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    # Download dataset\n",
    "    download_path = \"kaggle_data\"\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    api.dataset_download_files('rounakbanik/the-movies-dataset', path=download_path, unzip=True)\n",
    "\n",
    "    dataset_file = os.path.join(download_path, \"ratings.csv\")\n",
    "\n",
    "    # Set up Spark\n",
    "    sc = SparkContext(\"local\", \"MovieRatings\")\n",
    "\n",
    "    # Read the input data\n",
    "    input_rdd = sc.textFile(dataset_file)\n",
    "\n",
    "    # Parse and filter the data\n",
    "    mapped_rdd = input_rdd.filter(lambda line: not line.startswith(\"userId,movieId,rating,timestamp\")) \\\n",
    "                          .map(parse_line)\n",
    "\n",
    "    # Remove None values\n",
    "    mapped_rdd = mapped_rdd.filter(lambda x: x is not None)\n",
    "\n",
    "    # Calculate average ratings\n",
    "    reduced_rdd = mapped_rdd.groupByKey().mapValues(lambda ratings: sum(ratings) / len(ratings))\n",
    "\n",
    "    # Collect and print\n",
    "    results = reduced_rdd.collect()\n",
    "    for movie_id, avg_rating in results[:10]:  # Just print top 10 for demo\n",
    "        print(f\"Movie {movie_id} has an average rating of {avg_rating:.2f}\")\n",
    "\n",
    "    sc.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
